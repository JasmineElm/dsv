# Glossary

This glossary contains definitions of key terms and concepts related to data
science, data analysis, and data management. It is intended to help readers
understand and interpret technical terms and jargon commonly used in the field
of data science. The glossary is organized alphabetically by term, with each
term followed by a brief definition and explanation.

<!-- TODO: complete glossary once first draft complete -->

## A

### API

An API (Application Programming Interface) is a set of rules and protocols that
allows different software applications to communicate with each other. APIs are
commonly used to enable integration between different systems, allowing them to
share data and functionality. APIs are used in a wide range of applications,
from web development to mobile apps to data analysis. When we talk about APIs,
we are usually referring to web APIs, which allow applications to interact with
web servers. Web APIs are typically used to send and receive data from a server,
and are often used to build web applications.

## B

### Boolean

A boolean (or `bool`) is a data type that can have one of two values: `True` or
`False`. Booleans are commonly used in programming to represent whether a
condition is true or false. Booleans are named after the mathematician George
Boole, who developed the mathematical theory of logic that underlies their use.

### Blob

A blob (Binary Large Object) is a data type used to store large binary data,
such as images, audio files, or video files. Blobs are typically used in
databases to store binary data such as image, sound, video that is too large to
be stored in a regular text field.

## C

### CRISP-DM

CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is a
widely used methodology for data mining projects. The CRISP-DM framework is a
cyclical process that consists of six phases: Business Understanding, Data
Understanding, Data Preparation, Modeling, Evaluation, and Deployment. The
CRISP-DM framework provides a structured approach to conducting data mining
projects and ensures that all aspects of the project are considered. see also
[SEMMA](#SEMMA), [PPDAC](#PPDAC) and [KDD](#KDD)

## D

### Data Dictionary

A data dictionary is a collection of metadata that describes the structure,
content, and properties of a dataset. It provides information about the data
elements, their relationships, and their meanings. A data dictionary can help
users understand and interpret the data in a dataset, and can be used to ensure
consistency and accuracy in data analysis and reporting.

### Data Literacy

Data literacy is the ability to read, understand, create, and communicate data
as information. It involves the skills and knowledge needed to work with data
effectively, including the ability to interpret data, analyze data, and draw
meaningful insights from data. Data literacy is an important skill in the
digital age, as data is increasingly used to inform decision-making and solve
complex problems.

### Data Cube

A data cube is a way of representing multidimensional data. The Scottish
Statistics Agency has a good explanation of data cubes
[here](https://statistics.gov.scot/help/data_cubes)

### Data Mining

Data mining is the process of discovering patterns, trends, and insights in
large datasets. It involves using statistical and machine learning techniques to
extract valuable information from data. Data mining is commonly used in a wide
range of applications, from marketing to finance to healthcare. It can help
organizations make better decisions, optimize processes, and improve outcomes.

### Dataset

A dataset is a collection of data that is organized in a structured way.
Datasets can be stored in a variety of formats, including spreadsheets,
databases, and text files. They can contain a wide range of data types,
including numbers, text, images, and more. Datasets can be divided into two main
types: structured and unstructured. Structured datasets are highly organized and
can be easily searched and analysed. Unstructured datasets are less organized
and can be more difficult to search and analyze.

### Data Visualisation

Data visualisation is the graphical representation of data to help people better
understand the data. It involves creating visual representations of data, such
as charts, graphs, and maps, to communicate information and insights. Data
visualisation is an essential tool in data analysis, as it can quickly identify
patterns, trends, and relationships in the data. It can also help to communicate
complex information in a clear and concise way.

### DDL

DDL stands for Data Definition Language. It is a subset of SQL (Structured Query
Language) that is used to define the structure of a database. DDL commands are
used to create, modify, and delete database objects such as tables, indexes, and
views. Common DDL commands include `CREATE TABLE`, `ALTER TABLE`, and
`DROP TABLE`.

### DML

DML stands for Data Manipulation Language. It is a subset of SQL (Structured
Query Language) that is used to manipulate data in a database. DML commands are
used to insert, update, delete, and retrieve data from database tables. Common
DML commands include `INSERT`, `UPDATE`, `DELETE`, and `SELECT`.

### Domain Knowledge

Domain knowledge is knowledge about a specific subject or field. It includes
knowledge of the terminology, concepts, and practices that are used in that
field. Domain knowledge is important in data science, as it can help data
scientists understand the data they are working with and interpret the results
of their analysis. Domain knowledge can come from a variety of sources,
including education, work experience, and personal interests.

## E

## F

## G

## H

## I

### Integer

An integer is a whole number that does not have a fractional or decimal part.
Integers can be positive, negative, or zero. Examples of integers include -3, 0,
and 42. In programming, integers (or `int`s) are commonly used to represent
counts, quantities, and identifiers.

## J

## K

### KDD

KDD stands for Knowledge Discovery in Databases. It is the process of
discovering useful knowledge from large volumes of data. The KDD process
involves several steps, including data selection, data preprocessing, data
transformation, data mining, and interpretation of the results. KDD is commonly
used in data mining and machine learning to extract valuable insights from data.

## L

## M

## N

### Network Analysis

Network analysis is the study of relationships between entities in a network. It
involves analysing the structure of the network, identifying patterns and
trends, and extracting insights from the data. Network analysis is commonly used
in a wide range of applications, from social networks to financial networks to
biological networks. It can help to uncover hidden relationships, identify key
players, and understand the flow of information or resources in a network.

## O

### Open Data

Open data is data that is freely available to the public to use, reuse, and
redistribute without restrictions. Open data is typically published in a
machine-readable format and is often accompanied by a license that specifies how
the data can be used. Open data is used in a wide range of applications, from
research to policy-making to citizen engagement. It can help to increase
transparency, accountability, and innovation in society.

### Ordinal Data

Ordinal data is a type of data that can be ordered or ranked. It is a
categorical data type that has a natural order, but the intervals between the
categories are not necessarily equal. For example, a survey question that asks
respondents to rate their satisfaction on a scale of 1 to 5 is an example of
ordinal data.

### Outliers

An outlier is an observation that is significantly different from other data.
Outliers can occur due to errors in data collection, measurement errors, or
natural variation in the data. Outliers can have a significant impact on data
analysis and can skew the results of statistical tests. It is important to
identify and handle outliers appropriately to ensure the accuracy and
reliability of the analysis.

## P

### PPDAC

PPDAC stands for Problem, Plan, Data, Analysis, and Conclusion. It is a
framework commonly used in data science to guide the research process. The PPDAC
framework helps to ensure that researchers are following a logical process and
considering all aspects of their research. See also [CRISP-DM](#CRISP-DM),
[SEMMA](#SEMMA) and [KDD](#KDD)

### Predictive Analytics

Predictive analytics is the practice of using data, statistical algorithms, and
machine learning techniques to identify the likelihood of future outcomes based
on historical data. It is commonly used in a wide range of applications, from
forecasting sales to predicting customer behaviour to detecting fraud.
Predictive analytics can help organizations make better decisions, optimize
processes, and improve outcomes.

### PROMPT

PROMPT stands for Presentation, Relevance, Objectivity, Method, Provenance, and
Timeliness. It is a set of criteria used to evaluate the quality of information.
Whilst typically a tool to evaluate texts, the PROMPT criteria can be a useful
tool for evaluating datasets as well. By applying the PROMPT criteria to a
dataset, you can assess its quality, relevance, objectivity, method, provenance,
and timeliness. This can help you determine whether the dataset is suitable for
your needs and whether it can be trusted to provide accurate and reliable
insights.

### Pseudonymised

Personal data that has been pseudonymised has had all identifying information
removed or replaced with a pseudonym. Pseudonymisation is a technique used to
protect the privacy of individuals by making it more difficult to identify them
from the data. It is often used in research and data analysis to comply with
data protection regulations.

## Q

### Qualitative Data

Qualitative data is data that is descriptive and non-numeric in nature. It
consists of observations, opinions, and other non-quantifiable information. It
is typically collected through interviews, surveys, and observations.
Qualitative data is commonly used in social sciences, market research, and other
fields where understanding human behaviour and attitudes is important.

### Quantitative Data

Quantitative data is data that is numerical in nature. It consists of
measurements, counts, and other quantifiable information. It is typically
collected through surveys, experiments, and other quantitative research methods.
Quantitative data is commonly used in fields such as statistics, economics, and
engineering, where precise measurements and analysis are required.

### Quality

Quality is a measure of how well a dataset meets the needs and expectations of
its users. It is a multidimensional concept that includes factors such as
accuracy, completeness, relevance, and timeliness. Quality is an important
consideration in data management, as it can affect the reliability and
usefulness of the data. Ensuring data quality is essential for making informed
decisions and drawing meaningful insights from data.

## R

### Relational Database

A relational database is a type of database that stores data in tables. Each
table consists of rows and columns, with each row representing a single record
and each column representing a different attribute of the record. Relational
databases are commonly used in applications that require complex data
relationships and transactions. They are typically accessed using a language
called SQL (Structured Query Language).

## S

### Semi-Structured Data

Semi-structured data is data that does not fit neatly into a structured format
but has some organizational properties. It is typically organized in a
hierarchical manner, with elements that have attributes or properties. Examples
of semi-structured data include JSON and XML files. Semi-structured data is
commonly found in web APIs and NoSQL databases.

### SEMMA

SEMMA stands for Sample, Explore, Modify, Model, and Assess. It is a data mining
methodology developed by SAS Institute that outlines the steps involved in the
data mining process. The SEMMA methodology helps to guide the data mining
process and ensure that the results are accurate and reliable. see also
[CRISP-DM](#CRISP-DM), [PPDAC](#PPDAC) and [KDD](#KDD)

### Structured Data

Structured data is data that is organized in a predefined manner. It is
typically organized in rows and columns, and can be easily searched and
analysed. It is the opposite of unstructured data, which is data that does not
have a predefined data model or is not organized in a predefined manner.

## T

## U

### Univarate Data

Univariate data is data that consists of a _single variable or attribute_. It is
the simplest form of data and is often used to describe the distribution of a
single variable. Univariate data analysis involves examining the distribution of
the data, identifying outliers, and summarizing the data using descriptive
statistics.

### Unstructured Data

Unstructured data is data that does not have a predefined data model or is not
organized in a predefined manner. It is typically text-heavy, but may contain
data such as dates, numbers, and facts as well. It is the opposite of structured
data, which is data that is organized in a predefined manner.

## V

## W

### Web Scraping

Web scraping is the process of automatically retrieving information from
websites. It involves extracting data from web pages and saving it in a
structured format for further analysis. Web scraping can be a useful technique
for collecting data from websites that do not provide APIs or have limited data
access. However, web scraping can be fragile and may break if the website
structure changes. It is important to be respectful of websites' terms of
service and use web scraping responsibly.

## X

## Y

## Z
