# Creating Your Own Datasets

Last week we looked at how to find and evaluate existing datasets.  This week we will look at how to create your own datasets.
## Useful links

https://www.linkedin.com/learning/sql-essential-training-20685933/

https://mystery.knightlab.com/


## Steps


# Manipulating Data

When analyzing univariate data, we focus on its overall shape and characteristics. Key questions include:

+ Location and spread of data points: What are the typical, minimum, and maximum values?
+ Distribution pattern: Are data points evenly spread or clustered?
+ Data set size: Is it large or small?
+ Symmetry: Is the distribution symmetric or skewed?
+ Tail weight: Are there many points far from the center, or are outliers rare?
+ Clusters: Number, location, and size of any clusters.
+ Outliers: Presence of significantly different data points.
+ Other features: Any unusual aspects like gaps, sharp cutoffs, or peculiar values.
Even simple data sets can reveal complex features.
# Week 4: Assessment Unpacking and Data Manipulation
## Context

> The Numbers have no way of speaking for themselves. We speak for them. We
> imbue them with meaning.

@silverSignalNoiseArt2013

Context is the information that surrounds the data. It is the information that
tells us what the data is about. Fundamentally, data without context is
meaningless. Data with context is _information_ Context is essential both in
understanding the raw data, and presenting our findings. Without context, we
cannot hope to understand whether our analysis is accurate or not.

Often, the context is not explicit in the data. It is up to the data scientist
to find the context, and to use it to interrogate the data. This is why data
science is as much an art as a science. It is the art of finding the context,
and using it to interpret and present the data.

## Activity 4.1 How many trees are there on our planet?

This question was posed by David Spiegelhalter in his book "The Art of
Statistics" @spiegelhalterArtStatisticsLearning2019. How could we possibly
answer this? We could jump into looking at forests and working out the density
of trees, and extrapolating a value from that, but there is a more fundamental
first step, we need to know _what is a tree_? Fortunately for us, there are
definitions we can use.

> Trees are woody plants having a more or less erect perennial stem(s) capable
> of achieving at least 3 inches (7.6 cm) in diameter at breast height, or 5
> inches (12.7 cm) diameter at root collar and a height of 16.4 feet (5.0
> meters) at maturity in situ. @nelsonDefiningUnitedStates2020. Though disputed
> (e.g., @brokawDBH2000) this provides a starting point for our investigation.

## Reading Activity

Context is explored in the the first chapter of "Storytelling with Data: A Data
Visualization Guide For Professionals" by Cole Nussbaumer Knaflic. This book is
available in the library.

## Hypothesis Testing

# Ethics in Data Science

You'll cover fundamental ethics in other modules, but there are a couple of
ethical considerations that are particularly relevant to data science. These
are:

- asymmetries of information
- biases in data
- privacy and security

> What’s the most evil thing that can be done with this? [...]By asking the team
> to imagine what their impact could be if you abandon all constraints, you
> allow for a conversation that will help you identify opportunities that you
> would otherwise miss, and refine good ideas into great ones. We don’t want to
> build “evil” products, but subversive thinking is a good way to get outside
> the proverbial box. (Patil Data Driven)

### Asymmetries of Information

An asymmetry of information gives the party with more information an advantage
in a transaction. Asymmetries of information are when one party in a transaction
has more information than the other. This gives the party with more information
an advantage in the transaction. For instance, if you are buying a used car, the
seller knows more about the car than you do. This means that the seller has an
advantage in the transaction. This is an example of an asymmetry of information.

In [Week One](Week_1-Introduction.md) we considered the Financial Crisis
of 2008. No single factor caused to the financial crisis, and no single person
knew the risks. Instead it was a combination of asymmetries of information that
led to the crisis. The crisis was caused by this combination of partial truths:

**Borrowers** knew that they were taking on risky loans, but they believed that
they would be able to refinance or sell their homes before the rates went up.
**Lenders** knew that the loans they were offering were risky, but they believed
that the risk was spread across the financial system, so they would not be
affected if the loans went bad. **Investment Banks** knew that the MBS they were
buying were potentially risky, but they believed that the underlying mortgages
were sound. **Regulators** knew that the financial system was taking on more
risk, but they believed that the market would self-correct.

The lenders had more information than the borrowers, the investment banks had
more information than the lenders, and the regulators had more information than
the investment banks. This meant that the risks were not fully understood, and
the consequences were not fully appreciated.

#### Activity 5.1:

look up the seminal paper on this subject by George Akerlof, "The Market for
Lemons: Quality Uncertainty and the Market Mechanism" (1970). This paper is a
classic in the field of economics and is a clear example of how asymmetries of
information can affect markets.

https://doi-org.libezproxy.open.ac.uk/10.1016/B978-0-12-214850-7.50022-X

### Privacy and Security

Data science relies on data, and data is often personal. This means that data
science can raise privacy and security concerns. It is important to be aware of
these concerns and to take steps to protect privacy and security.

There are laws and regulations that govern the use of data. For instance the
GDPR (General Data Protection Regulation) seeks to limit the use of personal
data and to protect the privacy of individuals. It has

Whilst these uses are important, it is also important to remember that data
science can be used for good or for bad. It is important to be aware of the
ethical implications of data science and to use data science responsibly.
@zuboffAgeSurveillanceCapitalism2019 argues that data science is being used to
manipulate people and to control society. She argues that data science is being
used to create a new form of power that she calls "surveillance capitalism". It
is important to be aware of these issues and to use data science responsibly.

### now watch this:

https://www.youtube.com/watch?v=8HzW5rzPUy8

### Target's Predictive Analytics

In 2012, Forbes reported that Target (a major supermarket in the US) had sent a
teenager a coupon book for expectant mothers @hillHowTargetFigured2012. The
teenager's father was furious, but when he went to the store to complain, he
found out that his daughter was indeed pregnant. Target had used predictive
analytics to predict that the teenager was pregnant based on her shopping
habits. Whilst likely apocryphal @piatetskyDidTargetReally2014, this story
illustrates the power of predictive analytics and the ethical implications of
using data science.

### Activity:

The New York Times has a more nuanced take on this story. Read the article and
consider the ethical implications of using predictive analytics in this way.

https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html
## Evaluating Datasets

You may have come across the "PROMPT" criteria for evaluating texts. The PROMPT
criteria are a set of questions that can be used to evaluate the quality of a
text. The PROMPT criteria are:

- **Presentation**
  - Is the information presented clearly?
  - Is the language appropriate?
  - Is it succinct?
  - Can I understand it?
- **Relevance**
  - Does this information match my needs right now?
  - What is it mostly about?
- **Objectivity**
  - Is there bias in what you are reading?
  - Might the author/s have any hidden agendas? Have they been selective with
    their evidence?
  - Is the language used emotive?
  - Are opinions expressed?
  - Are there sponsors?
  - What are they selling? A particular product, a corporate view?
  - Is there contribution from different viewpoints by diverse authors to
    provide a balanced overview?
  - Are you selecting sources which confirm your own biases or seeking a broad
    range of perspectives on an issue?
- **Method**
  - Is it clear how any research was carried out?
  - How was data gathered?
  - If statistical data is presented, what is this based on?
  - Do researchers address any differences in outcomes between groups (e.g.,
    ethnic/racial groups)?
  - Were the methods appropriate, rigorous, etc.?
- **Provenance**
  - Is it clear who produced this information?
  - Where does it come from? Whose opinions are these?
  - Do you trust this source of information?
  - Are there references/citations that lead to further reading, and are they
    trustworthy sources?
- **Timeliness**
  - When was it produced or published? Do any of the sources reinforce
    stereotypes or represent other outdated views?
  - Is it current?
  - Has the climate/situation changed since this information was made available?
  - Is it still up to date?


### Activity 2.1.1
#### How can you use the PROMPT criteria to evaluate a dataset? (10 min)

#### Discussion
<!--TODO: add examples-->
# Identifying datasets

Data Scientists often have to choose between using existing datasets or
collecting their own data. This week we will look at how to find and evaluate
existing datasets.

## What is a dataset?

Before we start looking for datasets, it is important to understand what a data
set is. A dataset is a collection of data that is organised in a structured way.
Datasets can be stored in a variety of formats, including spreadsheets,
databases, and text files. They can contain a wide range of data types,
including numbers, text, images, and more.

Datasets can be divided into two main types: structured and unstructured.
Structured datasets are highly organised and can be easily searched and
analysed. Unstructured datasets are less organised and can be more difficult to
search and analyse.

## Data Types

The data we work with may be categorised into different types. The type of data
we are working with will determine the methods we use to analyse it. Here are
some common data types:

| Data type   | Description                               |
| ----------- | ----------------------------------------- |
| Categorical | Data that can be divided into categories. |
| Numerical   | Data that consists of numbers.            |
| Ordinal     | Data that can be ordered.                 |
| Text        | Data that consists of text.               |
| Image       | Data that consists of images.             |
| Time series | Data that is collected over time.         |

Within each of these types, there are many different subtypes. For example,
numerical data can be further divided into discrete and continuous data. Mixing
different data types can lead to problems when analysing the data, so it is
important to understand the different types of data that you are working with.

On a purely functional level, we are interested in `datatypes`. Are we looking
at a number, date or string. We saw the impact of the _wrong_ datatype in the
demonstration in Week 1. Most modelling or analysis tools will have an extended
range of datatypes, you should find that most have a combination of the
following

| Common programming name | Description                                               |
| ----------------------- | --------------------------------------------------------- |
| int                     | Integer, a whole number                                   |
| float                   | Floating point number, a number with a decimal point      |
| str                     | String, a sequence of characters                          |
| bool                    | Boolean, a binary value, either True or False             |
| datetime                | Date and time                                             |
| blob                    | Binary Large Object, a large binary object e.g., an image |

## How data is stored

Data is typically stored in either **structured** or **unstructured** formats.  In structured data, the data is organised in a way that makes it easy to search and analyse. Structured data is typically stored in databases or spreadsheets. Unstructured data, on the other hand, is not organised in a structured way and can be more difficult to search and analyse. Examples of unstructured data include text documents, images, and videos. There is a third, increasingly common type of data-storage called **semi-structured** data. Semi-structured data is data that does not fit neatly into a structured format but has some organisational properties. Examples of semi-structured data include JSON and XML files.  You will find semi-structured data in web APIs and NoSQL databases.

## Activity 2.0.1 Reading (1 hour)

Read **chapter two of A Hands On Introduction to Data Science by Chirag Shah**


# Glossary


## A

## B

## C

## D

## E

## F

## G

## H

## I

## J

## K

## L

## M

## N

## O

### Ordinal Data

Ordinal data is a type of data that can be ordered or ranked. It is a categorical data type that has a natural order, but the intervals between the categories are not necessarily equal. For example, a survey question that asks respondents to rate their satisfaction on a scale of 1 to 5 is an example of ordinal data.

## P

### Pseudonymised

Personal data that has been pseudonymised has had all identifying information removed or replaced with a pseudonym. Pseudonymisation is a technique used to protect the privacy of individuals by making it more difficult to identify them from the data. It is often used in research and data analysis to comply with data protection regulations.

## Q

## R

## S

### Structured Data

Structured data is data that is organized in a predefined manner. It is typically organized in rows and columns, and can be easily searched and analysed. It is the opposite of unstructured data, which is data that does not have a predefined data model or is not organized in a predefined manner.

## T

## U

### Unstructured Data

Unstructured data is data that does not have a predefined data model or is not organized in a predefined manner. It is typically text-heavy, but may contain data such as dates, numbers, and facts as well. It is the opposite of structured data, which is data that is organized in a predefined manner.

## V

## W

## X

## Y

## Z

# Data Science and Visualisation


This is a placeholder for course materials I am creating for a Data Science and Visualisation module.  This should be treated as _very_ alpha quality.

You probably want to jump straight into [Week 1](01/1.0.md).
### Case Study: The Housing Market Crash of 2008

In 2008, the housing market crash triggered a global financial crisis that led
to the worst recession since the Great Depression. It has been depicted in films
such as The Big Short and Inside Job. This crash

This housing market crash wasn't caused by a single factor, but rather a
combination of interconnected issues:

#### Housing Bubble

Easy access to credit and low interest rates inflated housing prices beyond
their true value, creating a bubble. The American hosing market had
traditionally been stable. Nate Silver reports that

> "After adjusting for inflation a $10,000 investment in a home in 1896 would be
> worth just $10,600 in 1996. The rate of return had been less in a century than
> the stock market typically produces in a year."

@silverSignalNoiseArt2013 [p. 30]

You can see the relative US house prices in the graph below. Note the sharp
increase in house prices starting in 2003, and the subsequent crash in 2008.

![US House Prices 1890 - 2017](Assets/houseprices.png)

#### Activity 1.2.1

##### Recreate the above graph (45 Minutes)

There is a simple dataset
[here](https://www.multpl.com/case-shiller-home-price-index-inflation-adjusted/table/by-month)
that you can use to recreate this graph. I've created an `csv` (Comma Separated
Values) file of the data in [activities](../Activities/1/houseprices.csv) You
can use Excel, Google Sheets, or any other tool you are comfortable with.

##### Discussion

See the recording for a discussion of how I created the graph.

#### Subprime Mortgages

Lenders offered risky loans to borrowers with poor credit history (subprime) at
adjustable interest rates. These borrowers struggled to afford payments when
rates went up. In the space of just three years, the share of the market for
subprime mortgages had grown from 8.3% of the market to 23.5% of the market.
This near-tripling of the market share meant that almost a quarter of mortgages
were at active risk of default.
![Subprime Market Share 1996 - 2008](Assets/subprime.png)

Image courtesy of @thefinancialcrisisinquirycommissionFINANCIALCRISISINQUIRY2011
[p. 70]

##### Activity 1.2.2 Extend your learning (45 Minutes)

There is an excellent, albeit very deep explanation of the subprime mortgage
crisis
[here](https://www.nber.org/system/files/working_papers/w14625/w14625.pdf)
Whilst the statistics are beyond the scope of this course, it will give a great
insight into the rigour of the analysis that data scientists and economists
undertake.

#### Mortgage-Backed Securities

One of the underlying reasons these risky mortgages were able to be offered was
the development of new financial products. Financial institutions such as Lehman
Brothers and Bear Stearns had developed new financial products that allowed them
to bundle these risky mortgages together and sell them as investments. These
Mortgage-Backed Securities (MBS) were sold to investors, spreading the risk but
also making it hard to assess the underlying quality of the loans. Each Mortgage
Backed Security is essentially a bet that the underlying mortgages will be
repaid. These bets were then bundled together and sold on to other investors.
This meant that the risk was spread across the financial system, but also meant
that it was hard to assess the underlying quality of the loans. Lehman Brothers
effectively bet thirty one times _on each mortgage_ being repaid
[@lioudisCollapseLehmanBrothers2024]. When the housing market crashed, the value
of these Mortgage Backed Securities plummeted, and financial institutions
buckled under the weight of bad debt. This triggered a domino effect that led to
the global financial crisis.

#### Predatory Lending

These MBS were a primary contributing factor to the increase in predatory
lending practices. Loan providers knew that they could sell on the risk, so they
were less concerned about the underlying quality of the loans. Alongside this,
mortgage brokers were incentivised to sell as many loans as possible, as they
received a commission for each loan they sold. 
Unethical lending practices included misleading borrowers about loan terms and
inflating their income to qualify for loans. This meant that they could offer
loans to people who were unlikely to be able to repay them. This also had the
effect of accelerating the housing bubble; borrowers who were previously unable
to afford properties were now able to buy them, which in turn accelerated the
rise in house prices.

#### Lax Regulation

Underlying all of these issues was a lack of regulation. The US government had
deregulated the financial sector in the 1980s and 1990s, which allowed financial
institutions to take on more risk. The government also failed to regulate the
mortgage industry, which allowed predatory lending practices to flourish. The
government also failed to regulate the financial products that were being
developed, which allowed the creation of complex financial products that were
difficult to understand and assess. Compounding this was the fact that salaries
at credit agencies were a third of those at investment banks, meaning that the
best talent ended up working for the banks, rather than the credit agencies. The
gamekeeper was effectively working for the poacher.

These factors all came together to create a ticking time bomb. When the housing
bubble burst, homeowners defaulted on their mortgages, the value of MBS
plummeted, and financial institutions buckled under the weight of bad debt. This
triggered a domino effect that led to the global financial crisis.

#### Could Data Science Have Prevented the 2008 Financial Crisis?

The underlying causes of the 2008 financial crisis were complex, and it was only
after the fact that the pieces of the puzzle came together. <!--TODO: add examples-->

Despite the complexity of the contributing factors and the hidden risks, data
science could have helped identify the warning signs of the impending crisis.
Here's how:

#### Activity 1.2.3 Library Search (1 hour)

Use the library search to find out more about the housing crisis. You could
start with the following search terms:

- Housing crisis 2008
- Subprime mortgages
- Mortgage-backed securities
- Predatory lending
- Lax regulation

While reviewing papers, consider the following questions:

- Are the results reproducible?
- Are the methods clear?
- Are the data sources accessible?
- Do they suggest different causes of the crisis?
## Asking the right questions

Not all problems are as complex as the 2008 financial crisis, but all problems
require a similar systematic approach to interrogate.

The starting point for any analysis is to develop an understanding what the
problem is that you are trying to solve. This may seem obvious, but it is often
the most difficult part of the process. Without this, it It is easy to get lost
in the data, and to lose sight of _meaning_.

@gutmanBecomingDataHead2021 [p. 4] suggesta series of questions to help define
the problem:

1. Why is this problem important?
2. Who does this problem affect?
3. What if we don’t have the right data?
4. When is the project over?
5. What if we don’t like the results?

Using these questions we can define the scope and scale of the project and
ensure that we are focusing on the right problem. Not only will this ensure we
focus on the right things, it will also make it easier to develop our analyses, and draw robust conclusions. As we progress through the course, we will return to these
questions to help us define the problems we are trying to solve.

### Reading:

Becoming a Data Head - How to Think, Speak, and Understand Data Science,
Statistics, and Machine Learning Chapter 1

### Activity 1.3.1

> what other questions could you consider when defining a data science problem?
> Add your thoughts to the discussion forum.

#### Discussion

When approaching a novel problem, it can be difficult to know where to start. It
is common to be working on a problem where the issue is clear to the
stakeholders, but we lack the nuance to understand the problem fully. In these
cases, it can be helpful to ask a series of questions to help us understand the
problem better. Here are some questions you could consider when defining a data
science problem:

#### Who are the stakeholders we can talk to to understand the problem?

As data scientists, it is our job to transform data into information. We can
only hope to do this if we have a deep understanding of the problem we are
trying to solve. A good first step is to understand the context of the problem.
Who believes it's a problem, why? How long has this problem occurred? What are
the consequences of the problem? In order to communicate our findings
effectively, we need to understand the problem from the perspective of the
stakeholders as well as the data.

We may find ourselves working on problems in unfamiliar domains. In these cases,
it can be helpful to talk to those affected by or involved in the problem. This
can help us to understand the problem from their perspective, and to ensure that
we are focusing on the right things. Stakeholders can also help us unpack
unfamiliar terminology and acronyms, and to understand the problem in the
context of the domain.

#### What data sources are available to us?

If we are lucky, there dataset openly available to us that contains all the data we could possibly need.  More commonly we will need to collect data from multiple sources, or to collect our own data.  In some cases, we may need to collect data from scratch.  In these cases, it is important to consider what data is available to us.  It may be that we can obtain customer data, but this is pseudonymised.  It may be that we can obtain data from a government department, but this is aggregated.  It may be that we can obtain data from a company, but this is incomplete.  It is important to consider what data is available to us, and to consider whether we need to collect more data, or to aggregate multiple datasets.

##### Do I have enough data to draw robust conclusions?

Whilst there may be vast datasets available, it is important to consider whether
you have _enough data to draw robust conclusions_. If you are working with a
small dataset, you may need to consider whether you need to collect more data,
aggregate with other datasets or if it is appropriate to draw robust conclusions
from the data you have.

##### What are the limitations of the data?

It is important to consider the limitations of the data you are working with.
For example, you may need to consider whether the data is representative of the
population you are interested in, whether the data is accurate, and whether the
data is up-to-date.

##### Do I need to aggregate the data in some way?

Online datasets are often purposefully aggregated to protect privacy. You should
consider whether the the problem you are trying to solve needs de-aggregated
data. If it does, perhaps you need to collect your own data, or aggregate
multiple datasets.

##### What is the quality of the data?

Often, raw data is messy and needs to be cleaned before it can be analysed. You
should consider whether the data you are working with is clean, or whether you
need to clean it before you can analyze it.
# Week 1: Introduction

Welcome to the course.  This course is designed to give you a practical introduction to data science and visualisation.  As a Masters-level course, it is expected that you will be able to work independently and to a high standard.  This course is designed to give you the skills you need to be able to work with data and to present your findings in a clear and compelling way.

The course is delivered in ten blocks.  Ideally students will study these over ten weeks rather than bite the whole thing off in a single week.

## [Week 1](1.1.md)

In week one, we will introduce the subject and give you a flavour of what you can expect.

## [Week 2](../02/2.0.md)

In week two, we will look at how to find and evaluate existing datasets.

## [Week 3](../03/3.0.md)

In week three, we will look at how to extract data from various sources.

## [Week 4](../04/4.0.md)

In week four, we will look at how to manipulate data, and unpack the first assessment.

## [Week 5](../05/5.0.md)

In week five, we will look at hypothesis testing.

## [Week 6](../06/6.0.md)

In week six, we will look at exploratory data analysis.

## [Week 7](../07/7.0.md)

In week seven, we will look at how to visualise data.

## [Week 8](../08/8.0.md)

In week eight, we will look at how to tell stories with data.

## [Week 9](../09/9.0.md)

In week nine, we will look at other tools that can be used in data science and visualisation.

## [Week 10](../10/10.0.md)


### First Things First

> Look at the module guide and the assessment briefs. Make sure you understand
> what is expected of you and what you need to do to pass the module. Make sure
> you understand the deadlines and the assessment criteria. Whilst the
> terminology may be unfamiliar at this stage, it should give you an
> understanding of what is required, when. If you have any questions, let your
> tutor know. It is a good idea to get the deadlines in your calendar now, so
> you can plan your time effectively.
# Conclusion

This week we have explored the meaning of data science, the key attributes of a data scientist, and the types of data data anlaysis.

<!--something about financial crisis-->


### Activity 1.4.1 Reading (1 hour)

The concepts we have looked at this week are explored in more depth in **chapter one of A Hands On Introduction to Data Science by Chirag Shah**. Read this chapter to deepen your understanding of the concepts we have covered this week.

## Activity 1.4.2 Reflection

Before we move on to the next week, take a moment to reflect on what you have learned this week. What is the most important thing you have learned? What are you still unsure about? What would you like to learn more about? Post your thoughts in the course forum.

# Introduction

Data Science is the study of data to find insights and trends. It is a
multidisciplinary field that uses techniques from statistics, machine learning,
and computer science to analyse and interpret complex data. Data visualisation
is the process of presenting data in a visual format, such as charts, graphs,
and maps, to help people understand the data and make informed decisions.

In this course we will explore the key concepts and techniques of data science
and data visualisation, and learn how to apply them to real-world problems. We
will cover topics such as data cleaning, data wrangling, data analysis, and data
visualisation. The course purposefully doesn't focus on specific tools or
programming languages, but rather on the underlying concepts and techniques that
are common to all data science and data visualisation projects. As a data
scientist, you may not have free-choice of tools, so it is important to
understand the underlying principles and techniques that are common to all data
science projects.

## What Is Data Science?

Data Science is an essential part tool in the modern economy. It underpins the
AI revolution and is used in a wide range of industries, from finance to
healthcare to marketing. Data Science underpins our ability to:

1. Understand our customers better
2. Make informed decisions
3. develop new products and services

<!--TODO: add examples-->


Though you may associate Data Science with big tech companies or finance, it is
increasingly essential to a broad range of industries. Universities will use
data science to track and understand student performance to improve teaching and
support. Healthcare providers use data science to improve patient outcomes.
Governments use data science to improve public services. Data science is
_everywhere_.

## Types of Data Science

Data Science can be divided into four main areas. These areas are not mutually
exclusive, and many data scientists will work across multiple areas.

![Types of Data Science](Assets/Analytics_types.png)

### Descriptive Analytics

Descriptive analytics uses data to describe _what_ has happened in the past. For
example, a company might use descriptive analytics to analyze sales data to
understand trends and patterns. You'll try your hand at descriptive analytics
later in this week.

### Diagnostic Analytics

Diagnostic analytics uses data to understand _why_ something happened. For
example, a company might use diagnostic analytics to analyze customer feedback
to understand why sales have dropped. You'll try your hand at diagnostic
analytics later in this course.

### Predictive Analytics

Predictive analytics uses historical data to predict _how_ likely something is
to happen in the future. For example, a company might use predictive analytics
to forecast sales or customer churn.

### Prescriptive Analytics

Prescriptive analytics uses data to recommend actions to achieve a desired
outcome. For example, a company might use prescriptive analytics to optimise its
supply chain or marketing strategy.

## What is a Data Scientist?

A data Scientist is someone who can produce the above types of analysis.  Being "good at data" is not enough, *the medium is not the message*. The best data scientists are able to: communicate their findings clearly and effectively, work with stakeholders to understand their needs, and develop solutions that meet those needs.  According to @davenportDataScientistSexiest2012 the traditional route into data science was through a deep understanding of statistics; the job was dominated by people with PhDs in theoretical physics or statistics. However, the field has evolved, and data scientists now come from a wide range of backgrounds, including computer science, engineering, and social sciences. The key skills of a data scientist are:

- **Technical skills**: Data scientists need to be proficient in programming languages such as Python or R, and have a good understanding of statistics and machine learning algorithms.
- **Mathematical skills**: Data scientists need to be able to analyse data and draw meaningful conclusions from it. Whilst software packages such as Excel, SPSS, R and Python can help, it is important to understand the underlying principles of statistics.
- **Communication skills**: Data scientists need to be able to communicate their findings to a non-technical audience. This might involve creating visualisations, writing reports, or giving presentations.

Whilst these are core skills for a data science, there are many other skills that are useful. For example, data scientists need to be able to work with stakeholders to understand their needs, and develop solutions that meet those needs. They also need to be able to work with large datasets, and have a good understanding of data wrangling and cleaning techniques. Data scientists also need to be able to work with uncertainty, and be able to make decisions based on incomplete or imperfect data. underpinning all of these skills is a curious mind, and a willingness to experiment and learn new things. Data science is a rapidly evolving field, and data scientists need to be able to adapt to new technologies and techniques.

<!--## Data Literacy-->

![The attributes of a Data Scientist](Assets/DataScientistAttributes.png)

Image courtesy of @patilDataDriven2015 [p. 3]


