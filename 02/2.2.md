## 2.2: Evaluating Datasets

As we have seen, there is a tremendous amount of data available at the click of
a button. However, not all data is created equal. In this section, we will look
at how to evaluate datasets to ensure that they are of high quality and suitable
for your needs.

You may have come across the "PROMPT" criteria for evaluating texts. The PROMPT
criteria are a set of questions that can be used to evaluate the quality of a
text. The PROMPT criteria are:

- **Presentation**
  - Is the information presented clearly?
  - Is the language appropriate?
  - Is it succinct?
  - Can I understand it?
- **Relevance**
  - Does this information match my needs right now?
  - What is it mostly about?
- **Objectivity**
  - Is there bias in what you are reading?
  - Might the author/s have any hidden agendas? Have they been selective with
    their evidence?
  - Is the language used emotive?
  - Are opinions expressed?
  - Are there sponsors?
  - What are they selling? A particular product, a corporate view?
  - Is there contribution from different viewpoints by diverse authors to
    provide a balanced overview?
  - Are you selecting sources which confirm your own biases or seeking a broad
    range of perspectives on an issue?
- **Method**
  - Is it clear how any research was carried out?
  - How was data gathered?
  - If statistical data is presented, what is this based on?
  - Do researchers address any differences in outcomes between groups (e.g.,
    ethnic/racial groups)?
  - Were the methods appropriate, rigorous, etc.?
- **Provenance**
  - Is it clear who produced this information?
  - Where does it come from? Whose opinions are these?
  - Do you trust this source of information?
  - Are there references/citations that lead to further reading, and are they
    trustworthy sources?
- **Timeliness**
  - When was it produced or published? Do any of the sources reinforce
    stereotypes or represent other outdated views?
  - Is it current?
  - Has the climate/situation changed since this information was made available?
  - Is it still up to date?

The PROMPT criteria can be a useful tool for evaluating datasets as well. By
applying the PROMPT criteria to a dataset, you can assess its quality,
relevance, objectivity, method, provenance, and timeliness. This can help you
determine whether the dataset is suitable for your needs and whether it can be
trusted to provide accurate and reliable insights.

### Activity 2.2.1

_Allow 30 minutes_

Consider how you might you use the PROMPT criteria to evaluate a dataset? Try to
consider each of the criteria in turn, and think about how they might apply to a
dataset. For example,

- how would you assess the presentation of the dataset?
- Is the data presented clearly?
- Is it easy to understand?
- Is the language appropriate?
- Are there any biases or hidden agendas in the data?
- Are the methods used to collect the data clear and appropriate?
- Is the provenance of the data clear?
- Is the data up-to-date and relevant to your needs?

#### Discussion

Looking back at my earlier example of the broadband data from India, I can see
that the dataset was reasonably presented. However, The data was overly simple,
and did not contain any additional information that would help to interpret it.
For instance, both the years, and count of subscribers were truncated. Do they
represent the start of the year, or the end? Why were the subscriber numbers
rounded? Does the rounding have any impact on the data? The dataset was also not
particularly clear, as there was no information on the source of the data, how
it was collected, or what it represented. This made it difficult to assess the
quality and reliability of the data. There were also two values for the year
2014; `2014` and `May'14`. Which one is the best one to use? The dataset was
useful for my immediate needs - _what does the growth of Broadband look like in
India?_, but it ends in 2014. The dataset was also not particularly timely, as
it only contained data up to 2014, a lot may have changed in the past decade.
Why is there no data for subsequent years? Overall, the dataset did not meet the
PROMPT criteria for quality, relevance, objectivity, method, provenance, and
timeliness. Whilst I could still use this information, I would need to be
cautious about the conclusions I draw from it.

## Data Quality

Once we have found a dataset that is relevant to our needs, we need to assess
its quality. Data quality is a measure of how well data meets the needs of its
users. Data quality is important because it helps us ensure the accuracy and
reliability of the insights that can be drawn from the data.

Data quality is a measure of how well data meets the needs of its users. Data
quality is important because it affects the accuracy and reliability of the
insights that can be drawn from the data. Poor data quality can lead to
incorrect conclusions and poor decision-making. There are many factors that can
affect data quality, including:

- **Accuracy**: How accurate is the data? Does it contain errors or
  inconsistencies?
- **Completeness**: Is the data complete? Are there missing values or fields?
- **Consistency**: Is the data consistent? Are there discrepancies between
  different sources of data?
- **Timeliness**: Is the data up-to-date? Is it relevant to the current
  situation?
- **Relevance**: Is the data relevant to the problem you are trying to solve?

Data quality can be assessed using a variety of techniques, including data
profiling, data cleansing, and data validation. Data profiling involves
analysing the data to identify patterns, anomalies, and inconsistencies. Data
cleansing involves correcting errors, removing duplicates, and filling in
missing values. Data validation involves checking the data against predefined
rules or constraints to ensure that it meets the required quality standards.

## Activity 2.2.2

_Allow 30 minutes_

What are some examples of poor data quality that you have encountered?

Try to think of some examples of poor data quality that you have encountered in
your own work or personal life. What were the consequences of the poor data
quality? How could the data quality have been improved? Examples could include
missing values, incorrect data, inconsistent data, or outdated data.

#### Discussion

I was recently working on a project to understand the use of sex (biological)
and gender (identified by the individual) in an organisation. The dataset I was
working with had a field for both sets, but had only collected gender
information in the past couple of years. This meant that the dataset was
incomplete and inconsistent. Exacerbating this, there were two additional
issues:

1. Gender information was being backfilled into the `sex` field. If someone
   identified as male, both their gender and sex were recorded as male. This
   meant that the data was not only incomplete, but also inaccurate.
2. whilst both fields were codified, the organisation did not have definitions
   for the terms `sex` and `gender`, so it was unclear what the data was
   actually representing.

## Data _Meaning_

What the data means is just as important as the data itself. Data meaning covers
a broad range of topics, from the data's context to its interpretation. Data
meaning is important because it helps us understand the data and use it
effectively. In Section 1.4, we touched on
[Domain Knowledge](../glossary.md#domain-knowledge), which is the knowledge of a
specific field or subject area. Domain knowledge gives us a deeper understanding
of the data and helps us interpret it correctly. For example, if we are working
with a dataset on healthcare, we need to understand medical terminology,
procedures, and practices to interpret the data correctly. Without this
knowledge, we may misinterpret the data and draw incorrect conclusions. Where we
don't have domain knowledge, we may need to consult with experts in the field /
business to help us interpret the data correctly. Depending on the business,
they may have an informal or formal process for this, such as a data governance
board, or a data steward. If we are lucky, we may also be able to access a data
dictionary. A data dictionary is a document that provides detailed information
about the data in a dataset. This "data about the data" is called _metadata_.

Depending on the dataset, the data dictionary may also include additional
information, such as the data's origin, how it was collected, and any
transformations that have been applied to it. A data dictionary is a valuable
resource for understanding the data in a dataset and ensuring that it is used
correctly. Depending on the source, the data dictionary may be a separate
document, or it may be included as part of the dataset itself. Depending on the
data governance of the organisation, the data dictionary may also contain
information about who "owns" the data, who is responsible for maintaining it,
and who is responsible for ensuring that it is used correctly. This can be
particularly important in regulated industries, where data quality and data
governance are critical.

A data dictionary typically includes the following information:

- **Field Name**: The name of the field in the dataset.
- **Description**: A description of the field and the data it contains.
- **Data Type**: The type of data in the field (e.g., text, number, date).
- **Format**: The format of the data in the field (e.g., the length of a text
  field, the number of decimal places in a number field).
- **Valid Values**: The valid values that the field can contain.
- **Missing Values**: How missing values are represented in the field.
- **Example**: An example of the data in the field.
- **Source**: The source of the data in the field.

There is no standard format for a data dictionary, and the information it
contains. You may find the information represented in a table:

| Field Name | Description                      | Data Type | Format | Valid Values | Missing Values | Example    | Source |
| ---------- | -------------------------------- | --------- | ------ | ------------ | -------------- | ---------- | ------ |
| ID         | Unique identifier for the record | `Integer` | 10     | 1-9999999999 | -1             | 1234567890 | System |
| Name       | Name of the person               | `VARCHAR` | 50     | A-Z, a-z     | NULL           | John Doe   | User   |
| Age        | Age of the person                | `Integer` | 3      | 0-150        | -1             | 30         | User   |

---

## Activity 2.2.3

_allow 20 minutes_

Given the above data dictionary, answer the following questions:

- What is the data type of the `ID` field?
  - would a value of `-2` be valid?
- What potential issues are there with the `Name` field?
- What potential issues are there with the `Age` field?

### Discussion

- The data type of the `ID` field is `Integer`.
  - A value of `-2` would not be valid, as the valid values for the `ID` field
    are `1-9999999999`. Whilst we have a value of `-1` for missing values, the
    valid range is `1-9999999999`, so `-2` would not be valid.
- the name field is a `VARCHAR` field, with a maximum length of 50 characters.
  The valid values are `A-Z, a-z`, but there is no mention of special
  characters, such as spaces, hyphens, or non-ASCII characters (such as `é` or
  `ñ`). This could cause issues if the data contains these characters, as they
  may not be valid. The field also allows for `NULL` values, which could cause
  issues if the field is required.
- The age field is an `Integer` field, with a maximum length of 3 characters.
  The valid values are `0-150`, but there is no mention of negative values. This
  would be an issue if the user were to enter a typo of `-1` The field is an
  integer, which may be too blunt, for instance someone could be `30.5` years
  old, or `30.25` years old. The choice of an age field may also be problematic,
  as it is a moving target. If the data is not updated regularly, the age field
  will become less accurate over time. A better choice would be to capture the
  date of birth, and calculate the age from that.

---

Equally, you may find the information in a more visual format, such as the
interface from [amundsen](https://www.amundsen.io/):

![Amundsen's interface](Assets/2.2/amundsen_interface.png)

You may even find your database has a built-in data dictionary tools, for
instance PostgreSQL's `\d` command:

![Postgres \d command](Assets/2.2/postgres_slash_d.png) (see
[this blog post](https://blog.rustprooflabs.com/2018/09/pg-data-dictionary) for
more information)

Data Governance is a particularly broad topic, and is outside the scope of this
course. If you are interested in taking your learning further, the
[DAMA-DMBOK](https://dama.org/content/body-of-knowledge) is a good place to
start.

---

## Activity 2.2.4

_Allow 1 hour_

Read the article "Getting Started Creating Data Dictionaries: How to Create a
Shareable Data Set" by Buchanan et al. (2021). The article is available here:
https://journals.sagepub.com/doi/full/10.1177/2515245920928007

The article introduces a new tool for creating data dictionaries, called _Data
Dictionary Creator_. What potential downsides do you see with this tool? How
could you mitigate these downsides?
